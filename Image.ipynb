{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDI9BZJZyHEP",
        "outputId": "7ccf6cd8-eba3-4164-ea15-e42a1aedeb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 images in synthetic_images\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 21s/step - accuracy: 0.4990 - loss: 0.6933 - val_accuracy: 0.5041 - val_loss: 0.6934\n",
            "Epoch 2/50\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.5039 - loss: 0.6932"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3s/step - accuracy: 0.5039 - loss: 0.6932 - val_accuracy: 0.5048 - val_loss: 0.6930\n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 11s/step - accuracy: 0.5039 - loss: 0.6931 - val_accuracy: 0.5491 - val_loss: 0.6925\n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5019 - loss: 0.6931 - val_accuracy: 0.5812 - val_loss: 0.6923\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20s/step - accuracy: 0.5006 - loss: 0.6931 - val_accuracy: 0.5720 - val_loss: 0.6920\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3s/step - accuracy: 0.5022 - loss: 0.6931 - val_accuracy: 0.5319 - val_loss: 0.6918\n",
            "Epoch 7/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 19s/step - accuracy: 0.5036 - loss: 0.6931 - val_accuracy: 0.5065 - val_loss: 0.6917\n",
            "Epoch 8/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5s/step - accuracy: 0.5026 - loss: 0.6932 - val_accuracy: 0.5076 - val_loss: 0.6918\n",
            "Epoch 9/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18s/step - accuracy: 0.5039 - loss: 0.6931 - val_accuracy: 0.5173 - val_loss: 0.6920\n",
            "Epoch 10/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3s/step - accuracy: 0.5025 - loss: 0.6931 - val_accuracy: 0.5346 - val_loss: 0.6921\n",
            "Epoch 11/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18s/step - accuracy: 0.5029 - loss: 0.6931 - val_accuracy: 0.5682 - val_loss: 0.6923\n",
            "Epoch 12/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5033 - loss: 0.6931 - val_accuracy: 0.5557 - val_loss: 0.6924\n",
            "Epoch 13/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 12s/step - accuracy: 0.5032 - loss: 0.6931 - val_accuracy: 0.5185 - val_loss: 0.6925\n",
            "Epoch 14/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5030 - loss: 0.6931 - val_accuracy: 0.5098 - val_loss: 0.6926\n",
            "Epoch 15/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18s/step - accuracy: 0.5029 - loss: 0.6931 - val_accuracy: 0.5054 - val_loss: 0.6927\n",
            "Epoch 16/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5042 - loss: 0.6931 - val_accuracy: 0.5047 - val_loss: 0.6928\n",
            "Epoch 17/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 19s/step - accuracy: 0.5033 - loss: 0.6931 - val_accuracy: 0.5044 - val_loss: 0.6929\n",
            "Epoch 18/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5032 - loss: 0.6931 - val_accuracy: 0.5044 - val_loss: 0.6929\n",
            "Epoch 19/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 19s/step - accuracy: 0.5035 - loss: 0.6931 - val_accuracy: 0.5046 - val_loss: 0.6929\n",
            "Epoch 20/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5028 - loss: 0.6931 - val_accuracy: 0.5052 - val_loss: 0.6929\n",
            "Epoch 21/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 11s/step - accuracy: 0.5031 - loss: 0.6931 - val_accuracy: 0.5074 - val_loss: 0.6929\n",
            "Epoch 22/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5034 - loss: 0.6931 - val_accuracy: 0.5081 - val_loss: 0.6929\n",
            "Epoch 23/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 21s/step - accuracy: 0.5032 - loss: 0.6931 - val_accuracy: 0.5077 - val_loss: 0.6929\n",
            "Epoch 24/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.5044 - loss: 0.6931 - val_accuracy: 0.5066 - val_loss: 0.6929\n",
            "Epoch 25/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 18s/step - accuracy: 0.5034 - loss: 0.6931 - val_accuracy: 0.5057 - val_loss: 0.6930\n",
            "Epoch 26/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.5026 - loss: 0.6931 - val_accuracy: 0.5056 - val_loss: 0.6930\n",
            "Epoch 27/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 19s/step - accuracy: 0.5035 - loss: 0.6931 - val_accuracy: 0.5053 - val_loss: 0.6930\n",
            "Epoch 28/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3s/step - accuracy: 0.5031 - loss: 0.6931 - val_accuracy: 0.5054 - val_loss: 0.6930\n",
            "Epoch 29/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18s/step - accuracy: 0.5029 - loss: 0.6931 - val_accuracy: 0.5061 - val_loss: 0.6930\n",
            "Epoch 30/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4s/step - accuracy: 0.5041 - loss: 0.6931 - val_accuracy: 0.5065 - val_loss: 0.6930\n",
            "Epoch 31/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 12s/step - accuracy: 0.5028 - loss: 0.6931 - val_accuracy: 0.5093 - val_loss: 0.6930\n",
            "Epoch 32/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5041 - loss: 0.6931 - val_accuracy: 0.5104 - val_loss: 0.6929\n",
            "Epoch 33/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 11s/step - accuracy: 0.5033 - loss: 0.6931 - val_accuracy: 0.5118 - val_loss: 0.6929\n",
            "Epoch 34/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5034 - loss: 0.6931 - val_accuracy: 0.5115 - val_loss: 0.6929\n",
            "Epoch 35/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 21s/step - accuracy: 0.5034 - loss: 0.6931 - val_accuracy: 0.5090 - val_loss: 0.6930\n",
            "Epoch 36/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3s/step - accuracy: 0.5019 - loss: 0.6931 - val_accuracy: 0.5094 - val_loss: 0.6930\n",
            "Epoch 37/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 19s/step - accuracy: 0.5036 - loss: 0.6931 - val_accuracy: 0.5091 - val_loss: 0.6929\n",
            "Epoch 38/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.5029 - loss: 0.6931 - val_accuracy: 0.5094 - val_loss: 0.6929\n",
            "Epoch 39/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 18s/step - accuracy: 0.5028 - loss: 0.6931 - val_accuracy: 0.5134 - val_loss: 0.6929\n",
            "Epoch 40/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.5041 - loss: 0.6931 - val_accuracy: 0.5118 - val_loss: 0.6929\n",
            "Epoch 41/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 19s/step - accuracy: 0.5040 - loss: 0.6931 - val_accuracy: 0.5061 - val_loss: 0.6930\n",
            "Epoch 42/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5s/step - accuracy: 0.5029 - loss: 0.6931 - val_accuracy: 0.5057 - val_loss: 0.6930\n",
            "Epoch 43/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 19s/step - accuracy: 0.5027 - loss: 0.6931 - val_accuracy: 0.5063 - val_loss: 0.6930\n",
            "Epoch 44/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.5037 - loss: 0.6931 - val_accuracy: 0.5061 - val_loss: 0.6930\n",
            "Epoch 45/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 11s/step - accuracy: 0.5037 - loss: 0.6931 - val_accuracy: 0.5052 - val_loss: 0.6930\n",
            "Epoch 46/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.5027 - loss: 0.6931 - val_accuracy: 0.5047 - val_loss: 0.6930\n",
            "Epoch 47/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 11s/step - accuracy: 0.5044 - loss: 0.6931 - val_accuracy: 0.5047 - val_loss: 0.6931\n",
            "Epoch 48/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5022 - loss: 0.6931 - val_accuracy: 0.5047 - val_loss: 0.6931\n",
            "Epoch 49/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18s/step - accuracy: 0.5029 - loss: 0.6931 - val_accuracy: 0.5050 - val_loss: 0.6931\n",
            "Epoch 50/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5051 - loss: 0.6931 - val_accuracy: 0.5051 - val_loss: 0.6931\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Validation accuracy: 0.51\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Parameters\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# Function to create synthetic medical images and masks\n",
        "def create_synthetic_data(image_dir, mask_dir, num_images=100):\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    os.makedirs(mask_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Create a synthetic grayscale image\n",
        "        image = np.random.randint(0, 256, (IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "        cv2.imwrite(os.path.join(image_dir, f'image_{i}.png'), image)\n",
        "\n",
        "        # Create a corresponding binary mask (random for demonstration)\n",
        "        mask = (image > 128).astype(np.uint8) * 255  # Simple thresholding for mask\n",
        "        cv2.imwrite(os.path.join(mask_dir, f'mask_{i}.png'), mask)\n",
        "\n",
        "# Create synthetic data\n",
        "image_dir = 'synthetic_images'\n",
        "mask_dir = 'synthetic_masks'\n",
        "create_synthetic_data(image_dir, mask_dir)\n",
        "\n",
        "# Define U-Net model\n",
        "def unet_model(input_size=(IMG_HEIGHT, IMG_WIDTH, 1)):\n",
        "    inputs = layers.Input(input_size)\n",
        "\n",
        "    # Contracting path\n",
        "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    # Bottleneck\n",
        "    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    # Expansive path\n",
        "    u6 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = layers.concatenate([u6, c4])\n",
        "    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = layers.concatenate([u7, c3])\n",
        "    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = layers.concatenate([u8, c2])\n",
        "    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = layers.concatenate([u9, c1])\n",
        "    c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "# Load data function\n",
        "def load_data(image_dir, mask_dir):\n",
        "    images = []\n",
        "    masks = []\n",
        "    image_files = glob(os.path.join(image_dir, '*.png'))  # Adjust the extension as needed\n",
        "\n",
        "    print(f'Found {len(image_files)} images in {image_dir}')\n",
        "\n",
        "    for img_path in image_files:\n",
        "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode='grayscale')\n",
        "        img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "        images.append(img)\n",
        "\n",
        "        mask_path = os.path.join(mask_dir, os.path.basename(img_path).replace('image', 'mask'))  # Assuming masks have the same name\n",
        "        if os.path.exists(mask_path):\n",
        "            mask = tf.keras.preprocessing.image.load_img(mask_path, target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode='grayscale')\n",
        "            mask = tf.keras.preprocessing.image.img_to_array(mask) / 255.0\n",
        "            masks.append(mask)\n",
        "        else:\n",
        "            print(f'Mask not found for {mask_path}')\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load your data\n",
        "images, masks = load_data(image_dir, mask_dir)\n",
        "\n",
        "# Proceed only if images and masks are loaded\n",
        "if images.size > 0 and masks.size > 0:\n",
        "    # Split the data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model = unet_model()\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with data augmentation\n",
        "    model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "              validation_data=(X_val, y_val),\n",
        "              epochs=EPOCHS,\n",
        "              steps_per_epoch=len(X_train) // BATCH_SIZE)\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_predictions = model.predict(X_val)\n",
        "    val_predictions = (val_predictions > 0.5).astype(np.uint8)\n",
        "\n",
        "    # Calculate accuracy (or any other metric)\n",
        "    accuracy = accuracy_score(y_val.flatten(), val_predictions.flatten())\n",
        "    print(f'Validation accuracy: {accuracy:.2f}')\n",
        "else:\n",
        "    print(\"No data to process. Please check your directories.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUFXcDYRWuFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}